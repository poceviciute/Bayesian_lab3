---
title: "732A91 Lab 3"
author: "Fanny Karelius (fanka300), Milda Poceviciute (milpo192)"
date: "3 maj 2018"
output:
  html_document: default
---

#Question 1: Normal model, mixture of normal model with semi-conjugate prior

##a) Normal model

The daily precipitation $y=\{y_1,...,y_n\}$ is assumed to be independently normally distributed, $y|\mu,\sigma^2\sim N(\mu,\sigma^2)$, where $\mu,\sigma^2$ are unknown. The priors $\mu\sim N(\mu_0,\tau_0^2)$ and $\sigma^2\sim Inv-\chi^2(\nu_0,\sigma_0^2)$ are independent.
The conditional posteriors are
$$\mu|\sigma^2,y\sim N(\mu_n,\tau_n^2)$$
and
$$\sigma^2|\mu,y\sim Inv-\chi^2(\nu_n,\frac{\nu_0\sigma_0^2+\sum_{i=1}^n(y_i-\mu)^2}{\nu_0+n})$$
where $\mu_n=w\bar{y}+(1-w)\mu_0$, $w=\frac{n/\sigma^2}{\frac{n}{\sigma^2}+\frac{1}{\tau_0^2}}$, $\frac{1}{\tau_n^2}=\frac{n}{\sigma^2}+\frac{1}{\tau_0^2}$ and $\nu_n=\nu_0+n$.

###i)

Gibbs sampling implementation:
```{r}
gibbs_sampler <- function(iter, data, mu0, tau20, nu0, sigma20){
  N <- nrow(data)
  x_bar <- mean(data[,1])
  nun <- nu0+N
  mu <- rnorm(1,mu0, sqrt(tau20))
  sigma2 <- nu0*sigma20/rchisq(1,nu0)
  results <- matrix(ncol=2,nrow=iter+1)
  results[1,1]<-mu
  results[1,2]<-sigma2
  colnames(results)<-c("mu", "sigma2")
  
  for(i in 1:iter){
    w <- (N/results[i,2])/((N/results[i,2])+(1/tau20))
    mun <- w*x_bar+(1-w)*mu0
    tau2n <- 1/((N/results[i,2])+(1/tau20))
    mu <- rnorm(1,mun, sqrt(tau2n))
    para_n <- (nu0*sigma20+sum((data$V1-mu)^2))/nun
    sigma2 <- nun*para_n/rchisq(1,nun) 
    results[i+1,]<-c(mu, sigma2)
  }
  results
}
```


###ii)
```{r}
rain<-read.table("Rainfall.dat",header=FALSE)

# Weakly informative priors based on our guesses about the possible paramter values
mu0 <-0
tau20 <-50
nu0 <-5
sigma20 <-20

burn_in <- 100
n <- 1000
rain_gibbs <- gibbs_sampler(n, rain, mu0, tau20, nu0, sigma20)
rain_gibbs <- as.data.frame(rain_gibbs)

post_mu <- mean(rain_gibbs$mu[burn_in:n])
post_sigma2 <- mean(rain_gibbs$sigma2[burn_in:n])
```

```{r,echo=FALSE}
plot(rain_gibbs$sigma2[burn_in:n], xaxt="n",type="l", xlab="Iteration", ylab="sigma2", main ="Gibbs sampling of sigma2")
axis(1, at=seq(0, (n-burn_in), by=50), labels=seq(burn_in, n, by=50))
```

```{r,echo=FALSE}
plot(rain_gibbs$mu[burn_in:n], xaxt="n", type="l", xlab="Iteration", ylab="mu", main ="Gibbs sampling of mu")
axis(1, at=seq(0,(n-burn_in), by=50), labels=seq(burn_in,n, by=50))

```

The plots of the trajectories of the sampled Markov chains of $\sigma^2$ and $\mu$ seem to be converging well. The Gibbs sampling takes large steps from iteration to iteration and the values do not appear to be (highly) correlated.

##b) Mixture normal model

```{r, results='hide', fig.keep='none'}
rawData <- rain
x <- as.matrix(rawData)

# Model options
nComp <- 2    # Number of mixture components

# Prior options
alpha <- 10*rep(1,nComp) # Dirichlet(alpha)
muPrior <- rep(0,nComp) # Prior mean of mu
tau2Prior <- rep(10,nComp) # Prior std of mu
sigma2_0 <- rep(var(x),nComp) # s20 (best guess of sigma2)
nu0 <- rep(4,nComp) # degrees of freedom for prior on sigma2

# MCMC options
nIter <- 1000 # Number of Gibbs sampling draws

# Plotting options
plotFit <- TRUE
# lineColors <- c("blue", "green", "magenta", 'yellow')
# sleepTime <- 0.1 # Adding sleep time between iterations for plotting
################   END USER INPUT ###############

###### Defining a function that simulates from the 
rScaledInvChi2 <- function(n, df, scale){
  return((df*scale)/rchisq(n,df=df))
}

####### Defining a function that simulates from a Dirichlet distribution
rDirichlet <- function(param){
  nCat <- length(param)
  piDraws <- matrix(NA,nCat,1)
  for (j in 1:nCat){
    piDraws[j] <- rgamma(1,param[j],1)
  }
  piDraws = piDraws/sum(piDraws) # Diving every column of piDraws by the sum of the elements in that column.
  return(piDraws)
}

# Simple function that converts between two different representations of the mixture allocation
S2alloc <- function(S){
  n <- dim(S)[1]
  alloc <- rep(0,n)
  for (i in 1:n){
    alloc[i] <- which(S[i,] == 1)
  }
  return(alloc)
}

# Initial value for the MCMC
nObs <- length(x)
S <- t(rmultinom(nObs, size = 1 , prob = rep(1/nComp,nComp))) # nObs-by-nComp matrix with component allocations.
mu <- matrix(ncol=nComp, nrow=nIter+1)
mu[1,] <- quantile(x, probs = seq(0,1,length = nComp))
sigma2 <- matrix(ncol=nComp, nrow=nIter+1)
sigma2[1,] <- rep(var(x),nComp)
probObsInComp <- rep(NA, nComp)

# Setting up the plot
xGrid <- seq(min(x)-1*apply(x,2,sd),max(x)+1*apply(x,2,sd),length = 100)
xGridMin <- min(xGrid)
xGridMax <- max(xGrid)
mixDensMean <- rep(0,length(xGrid))
effIterCount <- 0
hist_x <- max(invisible(hist(x)$density))
ylim <- c(0,2*hist_x)


for (k in 1:nIter){
  #message(paste('Iteration number:',k))
  alloc <- S2alloc(S) # Just a function that converts between different representations of the group allocations
  nAlloc <- colSums(S)
  #print(nAlloc)
  # Update components probabilities
  pi <- rDirichlet(alpha + nAlloc)
  
  # Update mu's
  for (j in 1:nComp){
    precPrior <- 1/tau2Prior[j]
    precData <- nAlloc[j]/sigma2[k,j]
    precPost <- precPrior + precData
    wPrior <- precPrior/precPost
    muPost <- wPrior*muPrior + (1-wPrior)*mean(x[alloc == j])
    tau2Post <- 1/precPost
    mu[k+1,j] <- rnorm(1, mean = muPost, sd = sqrt(tau2Post))
  }
  
  # Update sigma2's
  for (j in 1:nComp){
    sigma2[k+1,j] <- rScaledInvChi2(1, df = nu0[j] + nAlloc[j], scale = (nu0[j]*sigma2_0[j] + sum((x[alloc == j] - mu[k+1,j])^2))/(nu0[j] + nAlloc[j]))
  }
  
  # Update allocation
  for (i in 1:nObs){
    for (j in 1:nComp){
      probObsInComp[j] <- pi[j]*dnorm(x[i], mean = mu[k+1,j], sd = sqrt(sigma2[k+1,j]))
    }
    S[i,] <- t(rmultinom(1, size = 1 , prob = probObsInComp/sum(probObsInComp)))
  }
  
  # Printing the fitted density against data histogram
   if (plotFit && (k%%1 ==0)){
     effIterCount <- effIterCount + 1
  #   hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = paste("Iteration number",k), ylim = ylim)
     mixDens <- rep(0,length(xGrid))
  #   components <- c()
  for (j in 1:nComp){
       compDens <- dnorm(xGrid,mu[k+1,j],sd = sqrt(sigma2[k+1,j]))
       mixDens <- mixDens + pi[j]*compDens
  #     lines(xGrid, compDens, type = "l", lwd = 2, col = lineColors[j])
  #     components[j] <- paste("Component ",j)
  #   }
  mixDensMean <- ((effIterCount-1)*mixDensMean + mixDens)/effIterCount
  #   
  #   lines(xGrid, mixDens, type = "l", lty = 2, lwd = 3, col = 'red')
  #   legend("topleft", box.lty = 1, legend = c("Data histogram",components, 'Mixture'), 
  #          col = c("black",lineColors[1:nComp], 'red'), lwd = 2)
  #   Sys.sleep(sleepTime)
   }
  
   }
  
}
```

```{r}
for(i in 1:nComp){
  plot(mu[50:nIter,i], xlab="Iteration", ylab="mu", main=paste0("mu ", i), type="l")
}

for(i in 1:nComp){
  plot(sigma2[50:nIter,i], xlab="Iteration", ylab="sigma2", main=paste0("sigma2 ", i), type="l")
}
```

From the trajectory plots above we conclude that the $\mu$s and $\sigma^2$s of both components do not appear to converge well. The iterations seem to be very correlated.


##c)

```{r}
hist(x, breaks = 20, freq = FALSE, xlim = c(xGridMin,xGridMax), main = "Final fitted density")
lines(xGrid, mixDensMean, type = "l", lwd = 2, lty = 4, col = "red")
lines(xGrid, dnorm(xGrid, mean = post_mu, sd = sqrt(post_sigma2)), type = "l", lwd = 2, col = "blue")
legend("topright", box.lty = 1, legend = c("Data histogram","Mixture density","Normal density from a)"), col=c("black","red","blue"), lwd = 2)

```

Neither the normal density from a) nor the mixture model from b) seem to capture the distribution of the data well. The mixture model captures the highest peak better, but misses the rest of the data, whereas the normal density encapsulates most of the data except the highest peak.


#Question 2: Time series models in Stan

##a)

We simulate data $x_{1:T}$ from the AR(1)-process: $x_t = \mu + \phi(x_{t-1}-\mu)+\epsilon_t$, where $\epsilon_t \overset{iid}{\sim} N(0,\sigma^2)$ with $\mu=10$, $\sigma^2=2$, $x_1=\mu$, $T=200$ and $\phi\in\{-1,1\}$.

```{r,message=FALSE,warning=FALSE}
#Question 2

#a)

AR1sim <- function(n,mu,phi,sigma2){
  x <- vector(length=n)
  x[1] <- mu
  for(i in 2:n){
    epsilon <- rnorm(1, 0, sqrt(sigma2))
    x[i] <- mu + phi*(x[i-1]-mu) + epsilon
  }
  x
}

#init_para <- list(list(x=c(mu,rep(0,niter-1))))
#needs to be a list of a list since each chain needs its own list
par(mfrow=c(2,2))
fit1<-AR1sim(n=200,sigma2=2, phi=0.5, mu=10)
plot(fit1, type="l", xlab="t", ylab="x_t", main="AR(1), phi=0.5")

fit2<-AR1sim(n=200,sigma2=2, phi=0.9, mu=10)
plot(fit2, type="l", xlab="t", ylab="x_t", main="AR(1), phi=0.9")

fit3<-AR1sim(n=200,sigma2=2, phi=-0.9, mu=10)
plot(fit3, type="l", xlab="t", ylab="x_t", main="AR(1), phi=-0.9")

fit4<-AR1sim(n=200,sigma2=2, phi=0, mu=10)
plot(fit4, type="l", xlab="t", ylab="x_t", main="AR(1), phi=0")

```

The correlation between $x_t$ and $x_{t+h}$ is $\rho_h=\phi^h$, which means that larger $|\phi|$ gives larger correlation between observations $h$ lags apart.

##b)

Using the function in a) we simulate two AR(1)-processes $x_{1:T}$ with $\phi=0.3$ and $y_{1:T}$ with $\phi=0.95$. Then we estimate values of $\phi$, $\mu$ and $\sigma^2$ for these two processes using MCMC. The non-informative priors used were:
\newline
$\mu \sim N(0,10)$
\newline
$\sigma \sim Exp(0.1)$
\newline
$\phi \sim N(0,1)$
\newline
The priors were chosen so they would cover a wide range of values as well as the values that were used in generating AR(1)-process $x_{1:T}$ data.

###i)

```{r,message=FALSE,warning=FALSE, error=FALSE, include=FALSE}
#b)
library(rstan)

xT <- AR1sim(n=200, mu=10, phi=0.3, sigma2=2)
yT <- AR1sim(n=200, mu=10, phi=0.95, sigma2=2)

ar_mcmc = '
data {
  int<lower=0> iter;
  vector[iter] x;
}
parameters {
  real mu;
  real<lower=0> sigma;
  real<lower=-1, upper=1> phi;
}
model {
// Priors
  mu ~ normal(0,10);
  sigma ~ exponential(0.1); //very weak prior for sigma, but always postive
  phi ~ normal(0, 1);
// model
  for(i in 2:iter){
    x[i] ~ normal(mu + phi*(x[i-1]-mu), sigma);
  }
}'


burnin <- 100
niter <- 300
ndraws <- 200

xT_fit <- stan(model_code=ar_mcmc,
           data=list(iter=ndraws,x=xT),
           warmup=burnin,
           iter=niter,
           chains = 1)

#pairs(xT_fit)

para_postmean <- get_posterior_mean(xT_fit)[1:3]
mu_post2 <- extract(xT_fit)$mu
phi_post2 <- extract(xT_fit)$phi
sigma2_post2 <- (extract(xT_fit)$sigma)^2

perc1x <- sapply(as.data.frame(xT_fit), FUN=quantile, probs=0.025)[1:3]
perc1x[2]<-perc1x[2]^2
perc2x <- sapply(as.data.frame(xT_fit), FUN=quantile, probs=0.975)[1:3]
perc2x[2]<-perc2x[2]^2
n_eff_x <- summary(xT_fit)$summary[,"n_eff"][1:3]
```

```{r,echo=FALSE}
cat("Posterior means for mu, sigma2, phi: ", c(mean(mu_post2), mean(sigma2_post2), mean(phi_post2)))
cat("\n Upper limits for 95% credible interval: ", perc2x)
cat("\n Lower limits for 95% credible interval: ", perc1x)
cat("\n Number of effective posterior samples: ", n_eff_x)

```

From the simulations above we get quite close estimates for $\mu$. The estimate for $\sigma^2$ is higher than the value used in part a). It seems to be highly influenced by the sample variance of the $x_{1:T}$ data, which is `r var(xT)`. The 95% credible interval of $\phi$ includes the value of 0.3, but the mean is low.


```{r,message=FALSE,warning=FALSE, error=FALSE, include=FALSE}
yT_fit <- stan(model_code=ar_mcmc,
               data=list(iter=ndraws,x=yT),
               warmup=burnin,
               iter=niter,
               chains = 1)

Ypara_postmean <- get_posterior_mean(yT_fit)[1:3]

mu_post <- extract(yT_fit)$mu
phi_post <- extract(yT_fit)$phi
sigma2_post <- (extract(yT_fit)$sigma)^2

perc1y <- sapply(as.data.frame(yT_fit), FUN=quantile, probs=0.025)[1:3]
perc1y[2]<-perc1y[2]^2
perc2y <- sapply(as.data.frame(yT_fit), FUN=quantile, probs=0.975)[1:3]
perc2y[2]<-perc2y[2]^2
n_eff_y<- summary(yT_fit)$summary[,"n_eff"][1:3]
```

```{r,echo=FALSE}
cat("Posterior means for mu, sigma2, phi: ", c(mean(mu_post), mean(sigma2_post), mean(phi_post)))
cat("\n Upper limits for 95% credible interval: ", perc2y)
cat("\n Lower limits for 95% credible interval: ", perc1y)
cat("\n Number of effective posterior samples: ", n_eff_y)
```

From the simulations above we get quite close estimates for $\mu$, but the credibility interval is very wide. However, the estimate for $\sigma^2$ is higher than the value used in part a). The estimate of $\phi$ is close to the actual value and the credibility interval is quite narrow. 
 

###ii)

```{r}
# Trajectory plots of the mu 

plot(mu_post2, type='l')

plot(mu_post, type='l')

# Trajectory plots of the sigma2

plot(sigma2_post2, type='l')

plot(sigma2_post, type='l')

# Trajectory plots of the phi

plot(phi_post2, type='l')

plot(phi_post, type='l')
```

The trajectory plots of $\mu$, $\sigma^2$ and $\phi$ seem to have some correlation between the iterations, however they do not have a tendency to get stuck at certain values for a number of iterations. 


```{r}
plot(mu_post2, phi_post2, pch=19, xlab="mu", ylab="phi", main="Joint posterior of mu and phi for x")

plot(mu_post, phi_post, pch=19, xlab="mu", ylab="phi", main="Joint posterior of mu and phi for y")
```

From the plots above, we can see that the joint posterior with data $x_{1:T}$ is highly correlated. The joint posterior with data $y_{1:T}$ also has some trend in it, but at a small scale.

##c)

The number of infections $c_t$ at each time point follows an independent Poisson distribution, when conditioned on a latent AR(1)-process $x_t$:

$$c_t|x_t \sim Poisson(\exp(x_t)) $$
We are interested in estimating the latent intensity given by $\theta_t = \exp(x_t)$ over time.

The non-informative priors used were:
\newline
$\mu \sim N(0,10)$
\newline
$\sigma \sim Exp(1)$
\newline
$\phi \sim N(0,0.5)$
\newline
The priors were chosen so they would cover a wide range of values as well as the values that were used in generating AR(1)-process $x_{1:T}$ data.

```{r,message=FALSE,warning=FALSE, include=FALSE}
campy<-read.table("Campy.dat",header=TRUE)

campy_model = '
data {
  int<lower=0> iter;
  int c[iter];
  real<lower=1> lambda;
}
parameters {
  real mu;
  real<lower=0> sigma;
  real<lower=-1, upper=1> phi;
  real x[iter];
}

model {
  phi ~ normal(0,0.5);
  sigma ~ exponential(lambda);
  mu ~ normal(0,10);
  for(i in 2:iter){
    x[i] ~ normal(mu + phi*(x[i-1]-mu), sigma);
    c[i] ~ poisson(exp(x[i]));
  }
    
}'

c_fit <- stan(model_code=campy_model,
                     data=list(iter=140,c=campy$c, lambda=1),
                     warmup=30,
                     iter=140,
                     chains = 1)

#print(c_fit)
theta <- exp(get_posterior_mean(c_fit))[4:143]
perc1 <- exp(sapply(as.data.frame(c_fit)[,4:143], FUN=quantile, probs=0.025))
perc2 <- exp(sapply(as.data.frame(c_fit)[,4:143], FUN=quantile, probs=0.975))
```
```{r, echo=FALSE}
plot(theta, type="l", main="Campy c)")
lines(campy$c, col="red")
lines(perc1, col="blue")
lines(perc2, col="blue")
legend(x = 5, y=45, c("Data", "Theta", "95% CI"), col=c("red", "black", "blue"), lwd = 3)

```


The posterior mean of $\theta_t$ follows the general trend in the Campy data quite well. 95% credible interval seems to include majority of the data points.


##d)

Now we assume we have a belief that the prior of $\sigma^2$ should be smoother. Therefore, we use a shrinkage constant $\lambda$ to reduce $\epsilon_t$ value for each given time.

```{r,message=FALSE,warning=FALSE, include=FALSE}
d_fit <- stan(model_code=campy_model,
              data=list(iter=140,c=campy$c, lambda=100),
              warmup=30,
              iter=140,
              chains = 1)

#print(d_fit)
theta2 <- exp(get_posterior_mean(d_fit))[4:143]
perc1d <- exp(sapply(as.data.frame(d_fit)[,4:143], FUN=quantile, probs=0.025))
perc2d <- exp(sapply(as.data.frame(d_fit)[,4:143], FUN=quantile, probs=0.975))
```

```{r, echo=FALSE}
plot(theta2, type="l", main="Campy d)")
lines(campy$c, col="red")
lines(perc1d, col="blue")
lines(perc2d, col="blue")
legend(x = 5, y=40, c("Data", "Theta", "95% CI"), col=c("red", "black", "blue"), lwd = 3)

#Comparison:
plot(theta, type="l", main="Comparison c) and d)")
lines(theta2, col="red")
legend(x = 5, y=45, c("Theta c)", "Theta d)"), col=c("black", "red"), lwd = 3)


```

The posterior mean of $\theta_t$ in part d) does seem to be a bit smoother with prior $\sigma\sim\exp(\lambda = 100)$. The model in c) might be a better fit since it includes more data in the 95% credible interval than the 95% credible interval in d).

#Appendix
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```